{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buildlog Learning Analysis\n",
    "\n",
    "Analysis of buildlog's learning dynamics using the real data sources:\n",
    "- **SQLite tables**: `review_learnings`, `mistakes`, `reward_events`\n",
    "- **Signal log**: `~/.buildlog/emissions/signal.jsonl` (time-series backbone)\n",
    "- **Seed files**: Treatment intervention dates from filenames\n",
    "\n",
    "**Key metrics:**\n",
    "- `reinforcement_count` vs `contradiction_count` per learning → rule strength over time\n",
    "- `was_repeat` on mistakes → persistent blind spots\n",
    "- `corrected_by_rule` → direct positive attribution\n",
    "- `rules_active` on reward events → which rules were on during success/failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "BUILDLOG_DIR = Path.home() / \".buildlog\"\n",
    "DB_PATH = BUILDLOG_DIR / \"buildlog.db\"\n",
    "SIGNAL_LOG = BUILDLOG_DIR / \"emissions\" / \"signal.jsonl\"\n",
    "SEEDS_DIR = BUILDLOG_DIR / \"seeds\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load SQLite Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(DB_PATH)\n",
    "\n",
    "# Review learnings - the rule strength time series\n",
    "learnings = pd.read_sql_query(\"\"\"\n",
    "    SELECT * FROM review_learnings\n",
    "\"\"\", conn)\n",
    "learnings[\"first_seen\"] = pd.to_datetime(learnings[\"first_seen\"])\n",
    "learnings[\"last_reinforced\"] = pd.to_datetime(learnings[\"last_reinforced\"])\n",
    "\n",
    "# Mistakes\n",
    "mistakes = pd.read_sql_query(\"\"\"\n",
    "    SELECT * FROM mistakes\n",
    "\"\"\", conn)\n",
    "mistakes[\"timestamp\"] = pd.to_datetime(mistakes[\"timestamp\"])\n",
    "\n",
    "# Reward events\n",
    "rewards = pd.read_sql_query(\"\"\"\n",
    "    SELECT * FROM reward_events\n",
    "\"\"\", conn)\n",
    "rewards[\"timestamp\"] = pd.to_datetime(rewards[\"timestamp\"])\n",
    "rewards[\"rules_active\"] = rewards[\"rules_active\"].apply(\n",
    "    lambda x: json.loads(x) if x else []\n",
    ")\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(f\"Learnings: {len(learnings)}\")\n",
    "print(f\"Mistakes: {len(mistakes)}\")\n",
    "print(f\"Reward events: {len(rewards)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Rule Strength: Reinforcement vs Contradiction\n",
    "\n",
    "This is the key plot. Rules accumulating reinforcements are getting stronger.\n",
    "Rules with contradictions are being challenged. The ratio tells us which rules are reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary by category\n",
    "category_stats = learnings.groupby(\"category\").agg(\n",
    "    rules=(\"id\", \"count\"),\n",
    "    total_reinforcements=(\"reinforcement_count\", \"sum\"),\n",
    "    total_contradictions=(\"contradiction_count\", \"sum\"),\n",
    ").reset_index()\n",
    "category_stats[\"strength_ratio\"] = (\n",
    "    category_stats[\"total_reinforcements\"] / \n",
    "    (category_stats[\"total_reinforcements\"] + category_stats[\"total_contradictions\"] + 1)\n",
    ").round(2)\n",
    "category_stats = category_stats.sort_values(\"total_reinforcements\", ascending=False)\n",
    "category_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter: reinforcement vs contradiction per learning, colored by category\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "categories = learnings[\"category\"].unique()\n",
    "colors = plt.cm.tab10(range(len(categories)))\n",
    "color_map = dict(zip(categories, colors))\n",
    "\n",
    "for cat in categories:\n",
    "    subset = learnings[learnings[\"category\"] == cat]\n",
    "    ax.scatter(\n",
    "        subset[\"reinforcement_count\"], \n",
    "        subset[\"contradiction_count\"],\n",
    "        label=cat,\n",
    "        alpha=0.7,\n",
    "        s=50\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(\"Reinforcement Count\")\n",
    "ax.set_ylabel(\"Contradiction Count\")\n",
    "ax.set_title(\"Rule Strength: Reinforcements vs Contradictions\")\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Add diagonal line (equal reinforcement/contradiction)\n",
    "max_val = max(learnings[\"reinforcement_count\"].max(), learnings[\"contradiction_count\"].max())\n",
    "ax.plot([0, max_val], [0, max_val], 'k--', alpha=0.3, label='_nolegend_')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Learning Timeline\n",
    "\n",
    "When were rules first seen? When were they last reinforced?\n",
    "Gap between first_seen and last_reinforced shows rule longevity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative rules over time\n",
    "learnings_sorted = learnings.sort_values(\"first_seen\")\n",
    "learnings_sorted[\"cumulative_rules\"] = range(1, len(learnings_sorted) + 1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.plot(learnings_sorted[\"first_seen\"], learnings_sorted[\"cumulative_rules\"], linewidth=2)\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Cumulative Rules Learned\")\n",
    "ax.set_title(\"Rule Discovery Over Time\")\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%m-%d\"))\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule lifespan: days between first_seen and last_reinforced\n",
    "learnings[\"lifespan_days\"] = (\n",
    "    learnings[\"last_reinforced\"] - learnings[\"first_seen\"]\n",
    ").dt.total_seconds() / 86400\n",
    "\n",
    "print(\"Rule Lifespan Statistics (days):\")\n",
    "print(learnings[\"lifespan_days\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Mistake Analysis\n",
    "\n",
    "Which error classes have repeats? Which have attribution to rules?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(mistakes) > 0:\n",
    "    mistake_stats = mistakes.groupby(\"error_class\").agg(\n",
    "        total=(\"id\", \"count\"),\n",
    "        repeats=(\"was_repeat\", \"sum\"),\n",
    "        attributed=(\"corrected_by_rule\", lambda x: x.notna().sum()),\n",
    "    ).reset_index()\n",
    "    mistake_stats[\"repeat_rate\"] = (mistake_stats[\"repeats\"] / mistake_stats[\"total\"] * 100).round(1)\n",
    "    mistake_stats[\"attribution_rate\"] = (mistake_stats[\"attributed\"] / mistake_stats[\"total\"] * 100).round(1)\n",
    "    print(mistake_stats.to_string(index=False))\n",
    "else:\n",
    "    print(\"No mistakes in SQLite yet. Check emission files for pending data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Reward Events: Rule Activation\n",
    "\n",
    "Which rules were active during successful vs failed outcomes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outcome distribution\n",
    "print(\"Reward Outcomes:\")\n",
    "print(rewards[\"outcome\"].value_counts())\n",
    "print()\n",
    "\n",
    "# Average reward by outcome\n",
    "print(\"Average Reward Value by Outcome:\")\n",
    "print(rewards.groupby(\"outcome\")[\"reward_value\"].mean().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode rules_active to see which rules are most often active\n",
    "if rewards[\"rules_active\"].apply(len).sum() > 0:\n",
    "    rules_exploded = rewards.explode(\"rules_active\")\n",
    "    rules_exploded = rules_exploded[rules_exploded[\"rules_active\"].notna()]\n",
    "    \n",
    "    rule_outcomes = rules_exploded.groupby([\"rules_active\", \"outcome\"]).size().unstack(fill_value=0)\n",
    "    print(\"Rule Activation by Outcome:\")\n",
    "    print(rule_outcomes)\n",
    "else:\n",
    "    print(\"No rules_active data yet. Rules haven't been activated in tracked sessions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Signal Log: Time-Series Backbone\n",
    "\n",
    "The signal.jsonl file is the append-only event log. Every emission gets a line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals = []\n",
    "if SIGNAL_LOG.exists():\n",
    "    with open(SIGNAL_LOG) as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                signals.append(json.loads(line))\n",
    "\n",
    "signals_df = pd.DataFrame(signals)\n",
    "if len(signals_df) > 0:\n",
    "    signals_df[\"ts\"] = pd.to_datetime(signals_df[\"ts\"])\n",
    "    print(f\"Signal events: {len(signals_df)}\")\n",
    "    print(f\"Date range: {signals_df['ts'].min()} to {signals_df['ts'].max()}\")\n",
    "    print()\n",
    "    print(\"Event types:\")\n",
    "    print(signals_df[\"type\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Events over time\n",
    "if len(signals_df) > 0:\n",
    "    signals_df[\"hour\"] = signals_df[\"ts\"].dt.floor(\"h\")\n",
    "    hourly = signals_df.groupby([\"hour\", \"type\"]).size().unstack(fill_value=0)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    hourly.plot(kind=\"bar\", stacked=True, ax=ax, width=0.8)\n",
    "    ax.set_xlabel(\"Hour\")\n",
    "    ax.set_ylabel(\"Events\")\n",
    "    ax.set_title(\"Emission Events by Hour\")\n",
    "    ax.legend(title=\"Type\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Intervention Points: Seed Ingestion Dates\n",
    "\n",
    "Seed filenames have timestamps. These are the treatment dates for A/B analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse seed filenames for intervention dates\n",
    "interventions = []\n",
    "if SEEDS_DIR.exists():\n",
    "    for f in SEEDS_DIR.glob(\"*.yaml\"):\n",
    "        # Format: persona_name_2026-02-06T23-55-30.yaml\n",
    "        name = f.stem\n",
    "        parts = name.rsplit(\"_\", 1)\n",
    "        if len(parts) == 2:\n",
    "            persona, ts_str = parts\n",
    "            try:\n",
    "                ts = datetime.strptime(ts_str, \"%Y-%m-%dT%H-%M-%S\")\n",
    "                interventions.append({\"persona\": persona, \"ingested_at\": ts, \"file\": f.name})\n",
    "            except ValueError:\n",
    "                # Not a timestamped file\n",
    "                interventions.append({\"persona\": name, \"ingested_at\": None, \"file\": f.name})\n",
    "        else:\n",
    "            interventions.append({\"persona\": name, \"ingested_at\": None, \"file\": f.name})\n",
    "\n",
    "interventions_df = pd.DataFrame(interventions)\n",
    "if len(interventions_df) > 0:\n",
    "    print(\"Seed Files (Intervention Points):\")\n",
    "    print(interventions_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"No seed files found. Ingest qortex rules to create intervention points.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary: Current State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    \"learnings\": {\n",
    "        \"total\": len(learnings),\n",
    "        \"categories\": learnings[\"category\"].nunique(),\n",
    "        \"total_reinforcements\": int(learnings[\"reinforcement_count\"].sum()),\n",
    "        \"total_contradictions\": int(learnings[\"contradiction_count\"].sum()),\n",
    "    },\n",
    "    \"mistakes\": {\n",
    "        \"total\": len(mistakes),\n",
    "        \"with_attribution\": int(mistakes[\"corrected_by_rule\"].notna().sum()) if len(mistakes) > 0 else 0,\n",
    "    },\n",
    "    \"rewards\": {\n",
    "        \"total\": len(rewards),\n",
    "        \"with_rules_active\": int(rewards[\"rules_active\"].apply(len).gt(0).sum()),\n",
    "    },\n",
    "    \"signals\": len(signals_df) if len(signals_df) > 0 else 0,\n",
    "    \"interventions\": len(interventions_df) if len(interventions_df) > 0 else 0,\n",
    "}\n",
    "\n",
    "print(json.dumps(summary, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Next Steps\n",
    "\n",
    "**What we can measure now:**\n",
    "- Rule strength trends (reinforcement_count over time per learning)\n",
    "- Category-level evidence accumulation\n",
    "- Emission volume over time\n",
    "\n",
    "**What we need for causal attribution:**\n",
    "- `corrected_by_rule` populated on mistakes (direct attribution)\n",
    "- `rules_active` populated on reward events (which rules were on)\n",
    "- More seed ingestion events (intervention points)\n",
    "- Longer time series (days/weeks, not hours)\n",
    "\n",
    "**The experiment:**\n",
    "1. Ingest qortex rules for iterator_visitor_patterns, factory_patterns\n",
    "2. Continue normal work, accumulating mistakes and rewards\n",
    "3. Re-run this notebook after a week\n",
    "4. Compare: repeat rates before vs after intervention"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
