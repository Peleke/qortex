{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buildlog Learning Analysis\n",
    "\n",
    "Analysis of buildlog's learning dynamics using the real data sources:\n",
    "- **SQLite tables**: `review_learnings`, `mistakes`, `reward_events`\n",
    "- **Signal log**: `~/.buildlog/emissions/signal.jsonl` (time-series backbone)\n",
    "- **Seed files**: Treatment intervention dates from filenames\n",
    "\n",
    "**Key metrics:**\n",
    "- `reinforcement_count` vs `contradiction_count` per learning → rule strength over time\n",
    "- `was_repeat` on mistakes → persistent blind spots\n",
    "- `corrected_by_rule` → direct positive attribution\n",
    "- `rules_active` on reward events → which rules were on during success/failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "BUILDLOG_DIR = Path.home() / \".buildlog\"\n",
    "DB_PATH = BUILDLOG_DIR / \"buildlog.db\"\n",
    "SIGNAL_LOG = BUILDLOG_DIR / \"emissions\" / \"signal.jsonl\"\n",
    "SEEDS_DIR = BUILDLOG_DIR / \"seeds\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load SQLite Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(DB_PATH)\n",
    "\n",
    "# Review learnings - the rule strength time series\n",
    "learnings = pd.read_sql_query(\"\"\"\n",
    "    SELECT * FROM review_learnings\n",
    "\"\"\", conn)\n",
    "learnings[\"first_seen\"] = pd.to_datetime(learnings[\"first_seen\"])\n",
    "learnings[\"last_reinforced\"] = pd.to_datetime(learnings[\"last_reinforced\"])\n",
    "\n",
    "# Mistakes\n",
    "mistakes = pd.read_sql_query(\"\"\"\n",
    "    SELECT * FROM mistakes\n",
    "\"\"\", conn)\n",
    "mistakes[\"timestamp\"] = pd.to_datetime(mistakes[\"timestamp\"])\n",
    "\n",
    "# Reward events\n",
    "rewards = pd.read_sql_query(\"\"\"\n",
    "    SELECT * FROM reward_events\n",
    "\"\"\", conn)\n",
    "rewards[\"timestamp\"] = pd.to_datetime(rewards[\"timestamp\"])\n",
    "rewards[\"rules_active\"] = rewards[\"rules_active\"].apply(\n",
    "    lambda x: json.loads(x) if x else []\n",
    ")\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(f\"Learnings: {len(learnings)}\")\n",
    "print(f\"Mistakes: {len(mistakes)}\")\n",
    "print(f\"Reward events: {len(rewards)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Rule Strength: Reinforcement vs Contradiction\n",
    "\n",
    "This is the key plot. Rules accumulating reinforcements are getting stronger.\n",
    "Rules with contradictions are being challenged. The ratio tells us which rules are reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary by category\n",
    "category_stats = learnings.groupby(\"category\").agg(\n",
    "    rules=(\"id\", \"count\"),\n",
    "    total_reinforcements=(\"reinforcement_count\", \"sum\"),\n",
    "    total_contradictions=(\"contradiction_count\", \"sum\"),\n",
    ").reset_index()\n",
    "category_stats[\"strength_ratio\"] = (\n",
    "    category_stats[\"total_reinforcements\"] / \n",
    "    (category_stats[\"total_reinforcements\"] + category_stats[\"total_contradictions\"] + 1)\n",
    ").round(2)\n",
    "category_stats = category_stats.sort_values(\"total_reinforcements\", ascending=False)\n",
    "category_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter: reinforcement vs contradiction per learning, colored by category\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "categories = learnings[\"category\"].unique()\n",
    "colors = plt.cm.tab10(range(len(categories)))\n",
    "color_map = dict(zip(categories, colors))\n",
    "\n",
    "for cat in categories:\n",
    "    subset = learnings[learnings[\"category\"] == cat]\n",
    "    ax.scatter(\n",
    "        subset[\"reinforcement_count\"], \n",
    "        subset[\"contradiction_count\"],\n",
    "        label=cat,\n",
    "        alpha=0.7,\n",
    "        s=50\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(\"Reinforcement Count\")\n",
    "ax.set_ylabel(\"Contradiction Count\")\n",
    "ax.set_title(\"Rule Strength: Reinforcements vs Contradictions\")\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Add diagonal line (equal reinforcement/contradiction)\n",
    "max_val = max(learnings[\"reinforcement_count\"].max(), learnings[\"contradiction_count\"].max())\n",
    "ax.plot([0, max_val], [0, max_val], 'k--', alpha=0.3, label='_nolegend_')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Learning Timeline\n",
    "\n",
    "When were rules first seen? When were they last reinforced?\n",
    "Gap between first_seen and last_reinforced shows rule longevity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative rules over time\n",
    "learnings_sorted = learnings.sort_values(\"first_seen\")\n",
    "learnings_sorted[\"cumulative_rules\"] = range(1, len(learnings_sorted) + 1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.plot(learnings_sorted[\"first_seen\"], learnings_sorted[\"cumulative_rules\"], linewidth=2)\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Cumulative Rules Learned\")\n",
    "ax.set_title(\"Rule Discovery Over Time\")\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%m-%d\"))\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule lifespan: days between first_seen and last_reinforced\n",
    "learnings[\"lifespan_days\"] = (\n",
    "    learnings[\"last_reinforced\"] - learnings[\"first_seen\"]\n",
    ").dt.total_seconds() / 86400\n",
    "\n",
    "print(\"Rule Lifespan Statistics (days):\")\n",
    "print(learnings[\"lifespan_days\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Mistake Analysis\n",
    "\n",
    "Which error classes have repeats? Which have attribution to rules?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(mistakes) > 0:\n",
    "    mistake_stats = mistakes.groupby(\"error_class\").agg(\n",
    "        total=(\"id\", \"count\"),\n",
    "        repeats=(\"was_repeat\", \"sum\"),\n",
    "        attributed=(\"corrected_by_rule\", lambda x: x.notna().sum()),\n",
    "    ).reset_index()\n",
    "    mistake_stats[\"repeat_rate\"] = (mistake_stats[\"repeats\"] / mistake_stats[\"total\"] * 100).round(1)\n",
    "    mistake_stats[\"attribution_rate\"] = (mistake_stats[\"attributed\"] / mistake_stats[\"total\"] * 100).round(1)\n",
    "    print(mistake_stats.to_string(index=False))\n",
    "else:\n",
    "    print(\"No mistakes in SQLite yet. Check emission files for pending data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Reward Events: Rule Activation\n",
    "\n",
    "Which rules were active during successful vs failed outcomes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outcome distribution\n",
    "print(\"Reward Outcomes:\")\n",
    "print(rewards[\"outcome\"].value_counts())\n",
    "print()\n",
    "\n",
    "# Average reward by outcome\n",
    "print(\"Average Reward Value by Outcome:\")\n",
    "print(rewards.groupby(\"outcome\")[\"reward_value\"].mean().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode rules_active to see which rules are most often active\n",
    "if rewards[\"rules_active\"].apply(len).sum() > 0:\n",
    "    rules_exploded = rewards.explode(\"rules_active\")\n",
    "    rules_exploded = rules_exploded[rules_exploded[\"rules_active\"].notna()]\n",
    "    \n",
    "    rule_outcomes = rules_exploded.groupby([\"rules_active\", \"outcome\"]).size().unstack(fill_value=0)\n",
    "    print(\"Rule Activation by Outcome:\")\n",
    "    print(rule_outcomes)\n",
    "else:\n",
    "    print(\"No rules_active data yet. Rules haven't been activated in tracked sessions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Signal Log: Time-Series Backbone\n",
    "\n",
    "The signal.jsonl file is the append-only event log. Every emission gets a line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals = []\n",
    "if SIGNAL_LOG.exists():\n",
    "    with open(SIGNAL_LOG) as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                signals.append(json.loads(line))\n",
    "\n",
    "signals_df = pd.DataFrame(signals)\n",
    "if len(signals_df) > 0:\n",
    "    signals_df[\"ts\"] = pd.to_datetime(signals_df[\"ts\"])\n",
    "    print(f\"Signal events: {len(signals_df)}\")\n",
    "    print(f\"Date range: {signals_df['ts'].min()} to {signals_df['ts'].max()}\")\n",
    "    print()\n",
    "    print(\"Event types:\")\n",
    "    print(signals_df[\"type\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Events over time\n",
    "if len(signals_df) > 0:\n",
    "    signals_df[\"hour\"] = signals_df[\"ts\"].dt.floor(\"h\")\n",
    "    hourly = signals_df.groupby([\"hour\", \"type\"]).size().unstack(fill_value=0)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    hourly.plot(kind=\"bar\", stacked=True, ax=ax, width=0.8)\n",
    "    ax.set_xlabel(\"Hour\")\n",
    "    ax.set_ylabel(\"Events\")\n",
    "    ax.set_title(\"Emission Events by Hour\")\n",
    "    ax.legend(title=\"Type\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Intervention Points: Seed Ingestion Dates\n",
    "\n",
    "Seed filenames have timestamps. These are the treatment dates for A/B analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse seed filenames for intervention dates\n",
    "interventions = []\n",
    "if SEEDS_DIR.exists():\n",
    "    for f in SEEDS_DIR.glob(\"*.yaml\"):\n",
    "        # Format: persona_name_2026-02-06T23-55-30.yaml\n",
    "        name = f.stem\n",
    "        parts = name.rsplit(\"_\", 1)\n",
    "        if len(parts) == 2:\n",
    "            persona, ts_str = parts\n",
    "            try:\n",
    "                ts = datetime.strptime(ts_str, \"%Y-%m-%dT%H-%M-%S\")\n",
    "                interventions.append({\"persona\": persona, \"ingested_at\": ts, \"file\": f.name})\n",
    "            except ValueError:\n",
    "                # Not a timestamped file\n",
    "                interventions.append({\"persona\": name, \"ingested_at\": None, \"file\": f.name})\n",
    "        else:\n",
    "            interventions.append({\"persona\": name, \"ingested_at\": None, \"file\": f.name})\n",
    "\n",
    "interventions_df = pd.DataFrame(interventions)\n",
    "if len(interventions_df) > 0:\n",
    "    print(\"Seed Files (Intervention Points):\")\n",
    "    print(interventions_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"No seed files found. Ingest qortex rules to create intervention points.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary: Current State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    \"learnings\": {\n",
    "        \"total\": len(learnings),\n",
    "        \"categories\": learnings[\"category\"].nunique(),\n",
    "        \"total_reinforcements\": int(learnings[\"reinforcement_count\"].sum()),\n",
    "        \"total_contradictions\": int(learnings[\"contradiction_count\"].sum()),\n",
    "    },\n",
    "    \"mistakes\": {\n",
    "        \"total\": len(mistakes),\n",
    "        \"with_attribution\": int(mistakes[\"corrected_by_rule\"].notna().sum()) if len(mistakes) > 0 else 0,\n",
    "    },\n",
    "    \"rewards\": {\n",
    "        \"total\": len(rewards),\n",
    "        \"with_rules_active\": int(rewards[\"rules_active\"].apply(len).gt(0).sum()),\n",
    "    },\n",
    "    \"signals\": len(signals_df) if len(signals_df) > 0 else 0,\n",
    "    \"interventions\": len(interventions_df) if len(interventions_df) > 0 else 0,\n",
    "}\n",
    "\n",
    "print(json.dumps(summary, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Next Steps\n",
    "\n",
    "**What we can measure now:**\n",
    "- Rule strength trends (reinforcement_count over time per learning)\n",
    "- Category-level evidence accumulation\n",
    "- Emission volume over time\n",
    "\n",
    "**What we need for causal attribution:**\n",
    "- `corrected_by_rule` populated on mistakes (direct attribution)\n",
    "- `rules_active` populated on reward events (which rules were on)\n",
    "- More seed ingestion events (intervention points)\n",
    "- Longer time series (days/weeks, not hours)\n",
    "\n",
    "**The experiment:**\n",
    "1. Ingest qortex rules for iterator_visitor_patterns, factory_patterns\n",
    "2. Continue normal work, accumulating mistakes and rewards\n",
    "3. Re-run this notebook after a week\n",
    "4. Compare: repeat rates before vs after intervention"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 10. Fisher Manifold: Belief Trajectories in Information Space\n\nThe bandit maintains Beta(α, β) posteriors for each arm. As rewards arrive, these\nposteriors trace a path through the space of all Beta distributions.\n\nBut plotting (α, β) in Euclidean space is misleading: the \"true\" distance between\ntwo beliefs is measured by the **Fisher information metric**, not Euclidean distance.\n\n**Why it matters:**\n- Beta(1,1) → Beta(2,1) is a *huge* belief shift (ignorance → \"maybe good\")\n- Beta(100,100) → Beta(101,100) is a *tiny* shift (already very confident)\n- Euclidean distance is 1.0 in both cases. Fisher distance is wildly different.\n\nThe Fisher information metric for Beta(α, β) is:\n\n```\nG(α, β) = | ψ'(α) - ψ'(α+β)    -ψ'(α+β)         |\n           | -ψ'(α+β)            ψ'(β) - ψ'(α+β)   |\n```\n\nwhere ψ' is the trigamma function (scipy.special.polygamma(1, x)).\n\nThis section loads real bandit trajectories and visualizes them with\nFisher-aware metrics: speed, distance, and convergence.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import numpy as np\nfrom scipy.special import polygamma\nfrom collections import defaultdict\n\n# ── Fisher Information Metric for Beta(α, β) ──────────────────────────\n#\n# The trigamma function ψ'(x) = d²/dx² ln Γ(x) measures how\n# \"sensitive\" the distribution is to parameter changes at that point.\n\ndef fisher_metric(alpha, beta):\n    \"\"\"Compute the 2x2 Fisher information matrix for Beta(α, β).\n    \n    Returns G such that ds² = Σᵢⱼ Gᵢⱼ dθᵢ dθⱼ gives the\n    information-theoretic distance between nearby distributions.\n    \"\"\"\n    psi1_a = polygamma(1, alpha)       # ψ'(α)\n    psi1_b = polygamma(1, beta)        # ψ'(β)\n    psi1_ab = polygamma(1, alpha + beta)  # ψ'(α+β)\n    \n    G = np.array([\n        [psi1_a - psi1_ab,  -psi1_ab],\n        [-psi1_ab,           psi1_b - psi1_ab]\n    ])\n    return G\n\n\ndef fisher_speed(alpha1, beta1, alpha2, beta2):\n    \"\"\"Fisher speed between two consecutive posterior states.\n    \n    Computes ds = sqrt(Δθᵀ G(θ₁) Δθ) — the infinitesimal arc length\n    on the Fisher manifold. Uses the metric at the starting point.\n    \"\"\"\n    G = fisher_metric(alpha1, beta1)\n    dtheta = np.array([alpha2 - alpha1, beta2 - beta1])\n    \n    # ds² = Δθᵀ G Δθ\n    ds_sq = dtheta @ G @ dtheta\n    return np.sqrt(max(ds_sq, 0))  # Clamp for numerical safety\n\n\ndef posterior_entropy(alpha, beta):\n    \"\"\"Differential entropy of Beta(α, β) — measures uncertainty.\n    \n    Lower entropy = more concentrated belief = higher confidence.\n    As the system learns, entropy should decrease.\n    \"\"\"\n    from scipy.special import betaln\n    from scipy.special import digamma\n    \n    return (betaln(alpha, beta) \n            - (alpha - 1) * digamma(alpha) \n            - (beta - 1) * digamma(beta) \n            + (alpha + beta - 2) * digamma(alpha + beta))\n\n\nprint(\"Fisher metric at Beta(1,1) — uniform prior (maximum ignorance):\")\nprint(fisher_metric(1.0, 1.0).round(4))\nprint()\nprint(\"Fisher metric at Beta(10,10) — moderate confidence:\")\nprint(fisher_metric(10.0, 10.0).round(4))\nprint()\nprint(\"Fisher metric at Beta(100,100) — high confidence:\")\nprint(fisher_metric(100.0, 100.0).round(4))\nprint()\nprint(\"Notice: G shrinks as confidence grows.\")\nprint(\"A step of Δα=1 near Beta(1,1) covers MUCH more Fisher distance\")\nprint(f\"than near Beta(100,100): {fisher_speed(1,1,2,1):.4f} vs {fisher_speed(100,100,101,100):.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 10.1 Load Bandit Trajectories\n\nThe bandit state is an append-only JSONL file. Each line is a posterior snapshot\nafter an update. We reconstruct trajectories per (context, rule_id).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load bandit state — try project-local first, then global\nBANDIT_CANDIDATES = [\n    Path(\"../buildlog/bandit_state.jsonl\"),           # from qortex/notebooks/\n    Path.home() / \".buildlog\" / \"bandit_state.jsonl\",  # global\n    Path(\"../../buildlog-template/buildlog/bandit_state.jsonl\"),\n]\n\nbandit_path = None\nfor p in BANDIT_CANDIDATES:\n    if p.exists():\n        bandit_path = p\n        break\n\nif bandit_path is None:\n    print(\"No bandit_state.jsonl found. Checked:\")\n    for p in BANDIT_CANDIDATES:\n        print(f\"  {p}\")\n    trajectories = {}\nelse:\n    print(f\"Loading bandit state from: {bandit_path}\")\n    \n    # Parse JSONL — each line is a snapshot of one arm's posterior\n    snapshots = []\n    with open(bandit_path) as f:\n        for i, line in enumerate(f):\n            if line.strip():\n                record = json.loads(line)\n                record[\"step\"] = i  # Ordering proxy (append-only)\n                snapshots.append(record)\n    \n    bandit_df = pd.DataFrame(snapshots)\n    bandit_df[\"updated_at\"] = pd.to_datetime(bandit_df[\"updated_at\"])\n    \n    # Group into trajectories: (context, rule_id) → sequence of (α, β)\n    # Each trajectory starts at Beta(1,1) implicitly (the prior)\n    trajectories = {}\n    for (ctx, rid), group in bandit_df.groupby([\"context\", \"rule_id\"]):\n        group = group.sort_values(\"step\")\n        key = f\"{ctx}:{rid}\"\n        \n        # Prepend the implicit prior Beta(1,1)\n        points = [(1.0, 1.0)]\n        for _, row in group.iterrows():\n            points.append((row[\"alpha\"], row[\"beta\"]))\n        \n        trajectories[key] = points\n    \n    print(f\"\\nTrajectories loaded: {len(trajectories)}\")\n    for key, pts in trajectories.items():\n        a, b = pts[-1]\n        print(f\"  {key}: {len(pts)} points, current Beta({a:.2f}, {b:.2f}), \"\n              f\"mean={a/(a+b):.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 10.2 Trajectory Plot with Fisher Speed Coloring\n\nEach arm's posterior traces a path through (α, β) space. The color at each\nsegment encodes **Fisher speed** — how much *information-theoretic* distance\nthat update covered. Hot segments = large belief shifts. Cool segments = refinement.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from matplotlib.collections import LineCollection\nfrom matplotlib.colors import Normalize\n\nif trajectories:\n    fig, ax = plt.subplots(figsize=(10, 8))\n    \n    cmap = plt.cm.plasma\n    all_speeds = []\n    \n    # First pass: compute all speeds for normalization\n    for key, pts in trajectories.items():\n        for i in range(1, len(pts)):\n            a1, b1 = pts[i - 1]\n            a2, b2 = pts[i]\n            all_speeds.append(fisher_speed(a1, b1, a2, b2))\n    \n    if all_speeds:\n        norm = Normalize(vmin=0, vmax=max(all_speeds))\n    else:\n        norm = Normalize(vmin=0, vmax=1)\n    \n    # Second pass: plot each trajectory\n    markers = ['o', 's', 'D', '^', 'v', 'P', '*', 'X']\n    for idx, (key, pts) in enumerate(trajectories.items()):\n        alphas = [p[0] for p in pts]\n        betas = [p[1] for p in pts]\n        \n        # Compute per-segment Fisher speeds\n        speeds = [0.0]  # No speed for the first point\n        for i in range(1, len(pts)):\n            a1, b1 = pts[i - 1]\n            a2, b2 = pts[i]\n            speeds.append(fisher_speed(a1, b1, a2, b2))\n        \n        # Draw colored line segments\n        points = np.array(list(zip(alphas, betas)))\n        if len(points) > 1:\n            segments = np.array([[points[i], points[i + 1]] \n                                 for i in range(len(points) - 1)])\n            seg_speeds = speeds[1:]  # Speed at arrival point\n            \n            lc = LineCollection(segments, cmap=cmap, norm=norm, \n                               linewidths=2, alpha=0.8)\n            lc.set_array(np.array(seg_speeds))\n            ax.add_collection(lc)\n        \n        # Mark start and end\n        marker = markers[idx % len(markers)]\n        ax.plot(alphas[0], betas[0], marker=marker, color='gray', \n                markersize=10, markeredgecolor='black', linewidth=0,\n                label=f'{key} (start)', zorder=5)\n        ax.plot(alphas[-1], betas[-1], marker=marker, \n                color=cmap(norm(speeds[-1])), markersize=14,\n                markeredgecolor='black', markeredgewidth=1.5, \n                linewidth=0, zorder=5)\n        \n        # Label the endpoint\n        ax.annotate(key.split(\":\")[-1][:12], \n                    (alphas[-1], betas[-1]),\n                    textcoords=\"offset points\", xytext=(8, 5),\n                    fontsize=7, alpha=0.8)\n    \n    # Colorbar\n    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n    sm.set_array([])\n    cbar = plt.colorbar(sm, ax=ax, label=\"Fisher Speed (info-theoretic distance per update)\")\n    \n    # Reference lines\n    max_coord = max(max(p[0] for pts in trajectories.values() for p in pts),\n                    max(p[1] for pts in trajectories.values() for p in pts))\n    diag = np.linspace(0.5, max_coord + 0.5, 50)\n    ax.plot(diag, diag, 'k--', alpha=0.2, label='α = β (unbiased)')\n    \n    ax.set_xlabel(\"α (success evidence)\", fontsize=12)\n    ax.set_ylabel(\"β (failure evidence)\", fontsize=12)\n    ax.set_title(\"Belief Trajectories on Beta Parameter Space\\n(colored by Fisher speed)\", fontsize=13)\n    ax.set_xlim(0.5, max_coord + 0.5)\n    ax.set_ylim(0.5, max_coord + 0.5)\n    ax.set_aspect('equal')\n    plt.tight_layout()\n    plt.show()\n    \n    # Print summary\n    print(\"\\nFisher speed summary per trajectory:\")\n    for key, pts in trajectories.items():\n        speeds = []\n        for i in range(1, len(pts)):\n            a1, b1 = pts[i - 1]\n            a2, b2 = pts[i]\n            speeds.append(fisher_speed(a1, b1, a2, b2))\n        if speeds:\n            total = sum(speeds)\n            print(f\"  {key}: total path length = {total:.4f}, \"\n                  f\"max speed = {max(speeds):.4f}, \"\n                  f\"final speed = {speeds[-1]:.4f}\")\nelse:\n    print(\"No trajectories to plot.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 10.3 Pairwise Fisher Distance Between Arms\n\nHow \"far apart\" are the current beliefs about each arm? Arms with similar\nFisher positions have similar evidence profiles — they might be candidates\nfor merging or represent redundant rules. Arms that are far apart have\ndiverged significantly in the system's belief about their effectiveness.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from scipy.integrate import quad_vec\n\ndef fisher_distance_approx(a1, b1, a2, b2, n_steps=50):\n    \"\"\"Approximate Fisher (geodesic) distance via numerical path integral.\n    \n    Integrates ds along a straight line in parameter space.\n    Not the true geodesic, but a reasonable upper bound and good\n    approximation for nearby points.\n    \"\"\"\n    total = 0.0\n    for i in range(n_steps):\n        t0 = i / n_steps\n        t1 = (i + 1) / n_steps\n        a_mid = a1 + (a2 - a1) * (t0 + t1) / 2\n        b_mid = b1 + (b2 - b1) * (t0 + t1) / 2\n        \n        da = (a2 - a1) / n_steps\n        db = (b2 - b1) / n_steps\n        \n        G = fisher_metric(a_mid, b_mid)\n        dtheta = np.array([da, db])\n        ds = np.sqrt(max(dtheta @ G @ dtheta, 0))\n        total += ds\n    return total\n\nif trajectories and len(trajectories) > 1:\n    keys = list(trajectories.keys())\n    n = len(keys)\n    \n    # Get current (final) posterior for each arm\n    endpoints = {k: pts[-1] for k, pts in trajectories.items()}\n    \n    # Compute pairwise distances\n    dist_matrix = np.zeros((n, n))\n    for i in range(n):\n        for j in range(i + 1, n):\n            a1, b1 = endpoints[keys[i]]\n            a2, b2 = endpoints[keys[j]]\n            d = fisher_distance_approx(a1, b1, a2, b2)\n            dist_matrix[i, j] = d\n            dist_matrix[j, i] = d\n    \n    # Plot heatmap\n    fig, ax = plt.subplots(figsize=(8, 6))\n    \n    # Short labels for readability\n    short_labels = [k.split(\":\")[-1][:15] for k in keys]\n    \n    im = ax.imshow(dist_matrix, cmap='YlOrRd', aspect='auto')\n    ax.set_xticks(range(n))\n    ax.set_yticks(range(n))\n    ax.set_xticklabels(short_labels, rotation=45, ha='right', fontsize=9)\n    ax.set_yticklabels(short_labels, fontsize=9)\n    \n    # Annotate cells with distance values\n    for i in range(n):\n        for j in range(n):\n            color = 'white' if dist_matrix[i, j] > dist_matrix.max() * 0.6 else 'black'\n            ax.text(j, i, f'{dist_matrix[i, j]:.2f}', \n                    ha='center', va='center', fontsize=10, color=color)\n    \n    plt.colorbar(im, ax=ax, label='Approx. Fisher Distance')\n    ax.set_title('Pairwise Fisher Distance Between Arm Beliefs\\n(current posteriors)', fontsize=13)\n    plt.tight_layout()\n    plt.show()\n    \n    # Interpretation\n    max_idx = np.unravel_index(np.argmax(dist_matrix), dist_matrix.shape)\n    min_idx = None\n    min_val = float('inf')\n    for i in range(n):\n        for j in range(i + 1, n):\n            if dist_matrix[i, j] < min_val:\n                min_val = dist_matrix[i, j]\n                min_idx = (i, j)\n    \n    print(f\"\\nMost divergent pair: {keys[max_idx[0]]} ↔ {keys[max_idx[1]]} \"\n          f\"(distance = {dist_matrix[max_idx]:.4f})\")\n    if min_idx:\n        print(f\"Most similar pair:   {keys[min_idx[0]]} ↔ {keys[min_idx[1]]} \"\n              f\"(distance = {dist_matrix[min_idx]:.4f})\")\nelif trajectories:\n    print(\"Only one trajectory — need at least 2 for pairwise distances.\")\nelse:\n    print(\"No trajectories to compare.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 10.4 Entropy Convergence: Is the System Learning?\n\nPosterior entropy measures uncertainty. If the system is learning, entropy should\n**decrease over time** as evidence accumulates and beliefs sharpen.\n\nThe rate of decrease matters too: fast initial drops followed by plateaus suggest\nthe system quickly formed opinions and is now refining them. Entropy that\n*increases* would signal contradictory evidence — the system is getting *confused*.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "if trajectories:\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    colors = plt.cm.Set2(np.linspace(0, 1, len(trajectories)))\n    \n    # ── Left: Entropy over update steps ──\n    ax = axes[0]\n    for idx, (key, pts) in enumerate(trajectories.items()):\n        entropies = [posterior_entropy(a, b) for a, b in pts]\n        steps = range(len(pts))\n        ax.plot(steps, entropies, 'o-', color=colors[idx], \n                label=key.split(\":\")[-1][:15], linewidth=2, markersize=6)\n    \n    ax.set_xlabel(\"Update Step\", fontsize=11)\n    ax.set_ylabel(\"Posterior Entropy (nats)\", fontsize=11)\n    ax.set_title(\"Entropy Convergence per Arm\", fontsize=13)\n    ax.legend(fontsize=8, loc='best')\n    ax.axhline(y=posterior_entropy(1.0, 1.0), color='gray', linestyle=':', \n               alpha=0.5, label='_nolegend_')\n    ax.annotate(\"Beta(1,1) prior\", \n                xy=(0, posterior_entropy(1.0, 1.0)),\n                xytext=(0.5, posterior_entropy(1.0, 1.0) + 0.02),\n                fontsize=8, color='gray')\n    \n    # ── Right: Fisher speed over update steps ──\n    ax = axes[1]\n    for idx, (key, pts) in enumerate(trajectories.items()):\n        speeds = [0.0]  # First point has no speed\n        for i in range(1, len(pts)):\n            a1, b1 = pts[i - 1]\n            a2, b2 = pts[i]\n            speeds.append(fisher_speed(a1, b1, a2, b2))\n        steps = range(len(pts))\n        ax.plot(steps, speeds, 's-', color=colors[idx],\n                label=key.split(\":\")[-1][:15], linewidth=2, markersize=6)\n    \n    ax.set_xlabel(\"Update Step\", fontsize=11)\n    ax.set_ylabel(\"Fisher Speed\", fontsize=11)\n    ax.set_title(\"Learning Velocity per Arm\\n(should decrease as beliefs stabilize)\", fontsize=13)\n    ax.legend(fontsize=8, loc='best')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Numerical summary\n    print(\"Convergence summary:\")\n    print(f\"{'Arm':<35} {'H(start)':>10} {'H(end)':>10} {'ΔH':>10} {'Converging?':>12}\")\n    print(\"-\" * 80)\n    for key, pts in trajectories.items():\n        h_start = posterior_entropy(*pts[0])\n        h_end = posterior_entropy(*pts[-1])\n        delta = h_end - h_start\n        status = \"yes\" if delta < 0 else \"CONFUSED\" if delta > 0.01 else \"flat\"\n        short_key = key.split(\":\")[-1][:33]\n        print(f\"{short_key:<35} {h_start:>10.4f} {h_end:>10.4f} {delta:>+10.4f} {status:>12}\")\nelse:\n    print(\"No trajectories to analyze.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 10.5 Reading the Fisher Manifold\n\n**Trajectory plot (10.2):**\n- Each path starts at (1, 1) — the uniform prior, maximum ignorance\n- Movement **right** (↑α) = accumulating success evidence\n- Movement **up** (↑β) = accumulating failure evidence\n- **Hot colors** = the system's beliefs shifted dramatically on that update\n- **Cool colors** = refinement, diminishing marginal information\n\n**Distance heatmap (10.3):**\n- High distance = the system has strongly differentiated between two arms\n- Low distance = similar evidence profiles — possible redundancy\n- Zero diagonal = identity (same arm)\n\n**Entropy convergence (10.4):**\n- **Decreasing entropy** = the system is learning, beliefs are sharpening\n- **Flat entropy** = stasis — no new information arriving\n- **Increasing entropy** = contradictory signals (deserves investigation)\n- **Fisher speed decreasing** = the same Δα or Δβ covers less ground as confidence grows\n\n**What this enables for Singularity:**\n- The observability layer can render these plots in real-time as the bandit updates\n- Entropy convergence rate is a direct, quantitative answer to \"is the system learning?\"\n- Fisher distance between arms informs automated rule merging/pruning decisions\n- Speed anomalies (sudden spikes after plateau) flag regime changes worth investigating\n- All of this generalizes beyond rules to any arm: tools, prompts, models, strategies",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 11. Dynamical Systems: The Flow Field of Learning\n\nThe bandit update rule defines a **vector field** on (α, β) space. At every point,\nthere's an expected direction the posterior will move given the arm's true reward rate.\n\nFor an arm with true reward probability p:\n- E[Δα] = p (expected successes per trial)\n- E[Δβ] = 1 - p (expected failures per trial)\n\nThis vector field tells us: if you're standing at Beta(α, β) and the true rate is p,\nwhich way does the flow push you?\n\nWe can plot this TODAY. No geodesics, no Christoffel symbols needed.\nJust matplotlib quiver plots on real parameter space.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ── 11.1 Phase Portrait: Update Flow Field ──────────────────────────\n#\n# For a given true reward rate p, the expected update at (α, β) is:\n#   Δα = p,  Δβ = 1 - p\n#\n# The flow is CONSTANT in Euclidean coordinates (same vector everywhere).\n# But in Fisher coordinates, it covers different distances depending on\n# where you are. We show both: the Euclidean flow AND the Fisher-weighted\n# flow (arrows scaled by Fisher speed).\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\n\n# Grid for quiver plot\na_range = np.linspace(1.0, 6.0, 12)\nb_range = np.linspace(1.0, 6.0, 12)\nA, B = np.meshgrid(a_range, b_range)\n\nfor ax_idx, (p_true, title) in enumerate([\n    (0.8, \"Good arm (p=0.8)\"),\n    (0.5, \"Neutral arm (p=0.5)\"),\n    (0.2, \"Bad arm (p=0.2)\"),\n]):\n    ax = axes[ax_idx]\n    \n    # Expected update direction (constant in Euclidean coords)\n    DA = np.full_like(A, p_true)\n    DB = np.full_like(B, 1.0 - p_true)\n    \n    # Compute Fisher speed at each grid point for this update\n    speeds = np.zeros_like(A)\n    for i in range(A.shape[0]):\n        for j in range(A.shape[1]):\n            speeds[i, j] = fisher_speed(A[i,j], B[i,j], \n                                         A[i,j] + p_true, B[i,j] + 1 - p_true)\n    \n    # Scale arrows by Fisher speed (information-weighted flow)\n    # Normalize so arrows are visible\n    max_speed = speeds.max() if speeds.max() > 0 else 1\n    DA_scaled = DA * speeds / max_speed\n    DB_scaled = DB * speeds / max_speed\n    \n    # Plot Fisher-weighted flow\n    q = ax.quiver(A, B, DA_scaled, DB_scaled, speeds,\n                  cmap='plasma', alpha=0.8, scale=15, width=0.004)\n    \n    # Overlay real trajectories if we have them\n    if trajectories:\n        for key, pts in trajectories.items():\n            alphas = [pt[0] for pt in pts]\n            betas = [pt[1] for pt in pts]\n            ax.plot(alphas, betas, 'k-', alpha=0.3, linewidth=1)\n            ax.plot(alphas[-1], betas[-1], 'ko', markersize=4, alpha=0.5)\n    \n    # The fixed point of the MEAN dynamics: α/(α+β) = p\n    # This is the line α = p * (α + β), i.e., β = α * (1-p)/p\n    fp_a = np.linspace(1, 6, 50)\n    fp_b = fp_a * (1 - p_true) / p_true\n    mask = fp_b <= 6\n    ax.plot(fp_a[mask], fp_b[mask], 'r--', linewidth=2, alpha=0.6,\n            label=f'mean = {p_true} (attractor line)')\n    \n    ax.plot(1, 1, 'w*', markersize=15, markeredgecolor='black',\n            markeredgewidth=1.5, zorder=10, label='Prior Beta(1,1)')\n    ax.set_xlabel(\"α\", fontsize=11)\n    ax.set_ylabel(\"β\", fontsize=11)\n    ax.set_title(f\"{title}\\nArrow size = Fisher speed\", fontsize=12)\n    ax.set_xlim(0.5, 6.5)\n    ax.set_ylim(0.5, 6.5)\n    ax.legend(fontsize=8, loc='upper left')\n    ax.set_aspect('equal')\n\nplt.suptitle(\"Phase Portraits: Expected Update Flow (Fisher-weighted)\",\n             fontsize=14, y=1.02)\nplt.tight_layout()\nplt.show()\n\nprint(\"Key observations:\")\nprint(\"  - Arrows near (1,1) are LARGE: early updates cover huge Fisher distance\")\nprint(\"  - Arrows far from (1,1) are tiny: same Euclidean step, negligible information gain\")\nprint(\"  - The red dashed line is the attractor: where α/(α+β) = p_true\")\nprint(\"  - ALL trajectories flow toward the attractor. The manifold has no repellers.\")\nprint(\"  - The attractor is a LINE, not a point: concentration grows forever, mean stabilizes\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 11.2 Separatrix: The Geometry of Stubbornness\n\nIf two arms compete, there's a **separatrix** in belief space: on one side,\nthe system will eventually conclude \"arm A is better.\" On the other side, \"arm B is better.\"\n\nThe separatrix is the boundary of the basins of attraction. How much evidence\ndoes it take to cross it? That's a measure of how *stubborn* the system is:\nhow hard it is to change its mind once it's formed an opinion.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ── 11.2 Separatrix & Basins of Attraction ───────────────────────────\n#\n# For Thompson Sampling, arm A is preferred when its sampled value\n# exceeds arm B's. The DECISION BOUNDARY in belief space is where\n# the posterior means are equal: α_A/(α_A + β_A) = α_B/(α_B + β_B).\n#\n# But it's more nuanced than the mean: variance matters too.\n# A high-variance arm can still \"win\" a sample even with lower mean.\n# The TRUE separatrix depends on the full posterior overlap.\n#\n# For now: visualize the mean-based separatrix and the Fisher distance\n# to cross it from various starting points.\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# ── Left: Decision regions in (mean, concentration) space ──\nax = axes[0]\n\n# Transform trajectories to (mean, concentration) coordinates\n# mean = α/(α+β),  concentration = α+β\nif trajectories:\n    for idx, (key, pts) in enumerate(trajectories.items()):\n        means = [a / (a + b) for a, b in pts]\n        concs = [a + b for a, b in pts]\n        color = plt.cm.Set2(idx / max(len(trajectories) - 1, 1))\n        ax.plot(means, concs, 'o-', color=color, linewidth=2, markersize=5,\n                label=key.split(\":\")[-1][:15])\n        # Arrow showing direction of last step\n        if len(means) > 1:\n            ax.annotate('', xy=(means[-1], concs[-1]),\n                       xytext=(means[-2], concs[-2]),\n                       arrowprops=dict(arrowstyle='->', color=color, lw=2))\n\n# The separatrix in mean-concentration space is just mean = 0.5\n# (where two equal arms would be indistinguishable by mean)\nax.axvline(x=0.5, color='red', linestyle='--', linewidth=2, alpha=0.6,\n           label='mean = 0.5 (indifference)')\n\n# Shade basins\nax.axvspan(0, 0.5, alpha=0.05, color='blue')\nax.axvspan(0.5, 1, alpha=0.05, color='green')\nax.text(0.25, 0.5, '\"bad arm\"\\nbasin', ha='center', fontsize=10, \n        color='blue', alpha=0.5, transform=ax.get_xaxis_transform())\nax.text(0.75, 0.5, '\"good arm\"\\nbasin', ha='center', fontsize=10,\n        color='green', alpha=0.5, transform=ax.get_xaxis_transform())\n\nax.set_xlabel(\"Posterior Mean α/(α+β)\", fontsize=11)\nax.set_ylabel(\"Concentration α+β (total evidence)\", fontsize=11)\nax.set_title(\"Basins of Attraction\\n(mean-concentration coordinates)\", fontsize=12)\nax.legend(fontsize=8, loc='upper left')\nax.set_xlim(0, 1)\n\n# ── Right: Fisher distance to separatrix ──\nax = axes[1]\n\n# For each point on a grid, compute Fisher distance to the nearest\n# point on the separatrix (mean = 0.5 line, i.e., α = β)\na_range = np.linspace(1.0, 8.0, 20)\nb_range = np.linspace(1.0, 8.0, 20)\ndist_to_sep = np.zeros((len(b_range), len(a_range)))\n\nfor i, b_val in enumerate(b_range):\n    for j, a_val in enumerate(a_range):\n        # Nearest point on separatrix: same concentration, mean = 0.5\n        conc = a_val + b_val\n        a_sep = conc / 2\n        b_sep = conc / 2\n        dist_to_sep[i, j] = fisher_distance_approx(a_val, b_val, a_sep, b_sep)\n\nim = ax.imshow(dist_to_sep, extent=[1, 8, 1, 8], origin='lower',\n               cmap='RdYlGn', aspect='equal')\nplt.colorbar(im, ax=ax, label='Fisher distance to separatrix')\n\n# The separatrix itself (α = β diagonal)\nax.plot([1, 8], [1, 8], 'k--', linewidth=2, label='Separatrix (α = β)')\n\n# Overlay real trajectories\nif trajectories:\n    for key, pts in trajectories.items():\n        alphas = [p[0] for p in pts]\n        betas = [p[1] for p in pts]\n        ax.plot(alphas, betas, 'k-', alpha=0.5, linewidth=1.5)\n        ax.plot(alphas[-1], betas[-1], 'ko', markersize=5)\n\nax.set_xlabel(\"α\", fontsize=11)\nax.set_ylabel(\"β\", fontsize=11)\nax.set_title(\"Fisher Distance to Decision Boundary\\n(green = far from indifference = committed)\", fontsize=12)\nax.legend(fontsize=8)\n\nplt.tight_layout()\nplt.show()\n\n# Stubbornness metric: Fisher distance from current position to separatrix\nif trajectories:\n    print(\"\\nStubbornness (Fisher distance to separatrix):\")\n    for key, pts in trajectories.items():\n        a, b = pts[-1]\n        conc = a + b\n        d = fisher_distance_approx(a, b, conc/2, conc/2)\n        mean = a / (a + b)\n        side = \"good\" if mean > 0.5 else \"bad\" if mean < 0.5 else \"neutral\"\n        print(f\"  {key}: mean={mean:.3f} ({side}), \"\n              f\"Fisher dist to boundary={d:.4f}, \"\n              f\"evidence={conc:.1f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 12. Symmetry Checks: What's Conserved?\n\nBefore we can build the interoception layer, we need to know what the system's\nsymmetries actually ARE. Noether says: each symmetry = one conserved quantity.\nEach conserved quantity = one thing interoception should monitor.\n\nWe can verify three symmetries computationally right now:\n\n1. **Conjugation symmetry**: swapping α ↔ β is equivalent to relabeling\n   success/failure. The Fisher metric should be symmetric under this swap.\n\n2. **Scaling behavior**: doubling (α, β) → (2α, 2β) doesn't change the mean\n   but doubles the concentration. How does the metric scale?\n\n3. **Permutation symmetry breaking**: all arms start at Beta(1,1). As evidence\n   arrives, the pairwise distance matrix evolves. We can track HOW the symmetry\n   breaks: gradually or in sharp transitions.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ── 12.1 Conjugation Symmetry: G(α, β) vs G(β, α) ──────────────────\n#\n# If swapping α ↔ β just swaps the rows/columns of G, the metric\n# is conjugation-symmetric. This means the manifold looks the same\n# whether you call something \"success\" or \"failure.\"\n\nprint(\"=== Conjugation Symmetry Check ===\\n\")\n\ntest_points = [(1.0, 1.0), (2.0, 5.0), (3.7, 1.2), (10.0, 3.0)]\n\nfor a, b in test_points:\n    G_orig = fisher_metric(a, b)\n    G_swap = fisher_metric(b, a)\n    \n    # Under conjugation, G should transform as:\n    # G_swap[0,0] = G_orig[1,1]  (α-α component becomes β-β)\n    # G_swap[1,1] = G_orig[0,0]  (β-β becomes α-α)\n    # G_swap[0,1] = G_orig[0,1]  (off-diagonal unchanged)\n    \n    P = np.array([[0, 1], [1, 0]])  # Permutation matrix\n    G_conjugated = P @ G_orig @ P.T\n    \n    diff = np.abs(G_swap - G_conjugated).max()\n    status = \"EXACT\" if diff < 1e-12 else f\"BROKEN (diff={diff:.2e})\"\n    print(f\"  G({a}, {b}) vs G({b}, {a}): {status}\")\n\nprint(\"\\nConjugation symmetry holds: the manifold is symmetric about α = β.\")\nprint(\"Relabeling success/failure doesn't change the geometry.\")\nprint(\"→ This is why the separatrix IS the α = β diagonal.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ── 12.2 Scaling Behavior: How Does the Metric Change with Concentration? ──\n#\n# If we scale (α, β) → (kα, kβ), the mean stays the same but\n# concentration increases. How does the Fisher metric scale?\n# \n# If G(kα, kβ) = f(k) * G(α, β), the metric scales homogeneously.\n# The function f(k) tells us how \"information density\" changes with\n# evidence accumulation.\n\nprint(\"=== Scaling Behavior ===\\n\")\n\nbase_points = [(2.0, 3.0), (1.0, 1.0), (3.0, 1.0)]\nscale_factors = [1.0, 2.0, 5.0, 10.0, 50.0]\n\nfor a0, b0 in base_points:\n    print(f\"Base point ({a0}, {b0}), mean = {a0/(a0+b0):.3f}:\")\n    G0 = fisher_metric(a0, b0)\n    det_G0 = np.linalg.det(G0)\n    \n    for k in scale_factors:\n        Gk = fisher_metric(k * a0, k * b0)\n        det_Gk = np.linalg.det(Gk)\n        \n        # Ratio of determinants: det(Gk) / det(G0)\n        # If homogeneous: det(Gk) = k^p * det(G0) for some power p\n        if det_G0 > 0 and det_Gk > 0:\n            log_ratio = np.log(det_Gk / det_G0) / np.log(k) if k > 1 else 0\n            print(f\"  k={k:5.1f}: det(G) ratio = {det_Gk/det_G0:.6f}, \"\n                  f\"implied power ≈ {log_ratio:.2f}\")\n    print()\n\nprint(\"The metric scales approximately as k^(-2) along equal-mean lines.\")\nprint(\"Doubling all evidence → metric shrinks by ~4x → Fisher distances halve.\")\nprint(\"This is the geometric reason for diminishing returns:\")\nprint(\"the manifold literally compresses as evidence accumulates.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ── 12.3 Symmetry Breaking: How the Prior's Permutation Symmetry Dissolves ──\n#\n# At t=0, all arms are Beta(1,1). The pairwise Fisher distance matrix is\n# all zeros. As evidence arrives, arms differentiate. The total \"spread\"\n# of the distance matrix measures how much the original symmetry has broken.\n#\n# We track: Frobenius norm of the pairwise distance matrix at each step.\n# This is the symmetry-breaking curve.\n\nif trajectories and len(trajectories) > 1:\n    keys = list(trajectories.keys())\n    n_arms = len(keys)\n    \n    # Find the maximum trajectory length\n    max_len = max(len(pts) for pts in trajectories.values())\n    \n    # At each step, compute pairwise distances between all arms\n    # (using whatever state each arm is at, or its last known state)\n    symmetry_breaking = []\n    \n    for step in range(max_len):\n        # Get each arm's state at this step (or last known)\n        states = {}\n        for key, pts in trajectories.items():\n            idx = min(step, len(pts) - 1)\n            states[key] = pts[idx]\n        \n        # Pairwise Fisher distances\n        total_dist = 0.0\n        pairs = 0\n        for i in range(n_arms):\n            for j in range(i + 1, n_arms):\n                a1, b1 = states[keys[i]]\n                a2, b2 = states[keys[j]]\n                d = fisher_distance_approx(a1, b1, a2, b2, n_steps=20)\n                total_dist += d ** 2\n                pairs += 1\n        \n        frobenius = np.sqrt(total_dist)\n        symmetry_breaking.append({\n            'step': step,\n            'frobenius': frobenius,\n            'mean_dist': np.sqrt(total_dist / pairs) if pairs > 0 else 0,\n        })\n    \n    sb_df = pd.DataFrame(symmetry_breaking)\n    \n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    # Left: Symmetry-breaking curve\n    ax = axes[0]\n    ax.plot(sb_df['step'], sb_df['frobenius'], 'o-', color='darkred', \n            linewidth=2, markersize=6)\n    ax.set_xlabel(\"Update Step\", fontsize=11)\n    ax.set_ylabel(\"Frobenius Norm of Distance Matrix\", fontsize=11)\n    ax.set_title(\"Symmetry Breaking Over Time\\n(0 = all arms identical, ↑ = differentiated)\", \n                 fontsize=12)\n    ax.axhline(y=0, color='gray', linestyle=':', alpha=0.5)\n    ax.annotate(\"Perfect symmetry\\n(all arms at prior)\", xy=(0, 0),\n                xytext=(1, sb_df['frobenius'].max() * 0.3),\n                fontsize=9, color='gray',\n                arrowprops=dict(arrowstyle='->', color='gray'))\n    \n    # Right: Rate of symmetry breaking (first differences)\n    ax = axes[1]\n    if len(sb_df) > 1:\n        rates = sb_df['frobenius'].diff().fillna(0)\n        colors_rate = ['green' if r > 0 else 'blue' for r in rates]\n        ax.bar(sb_df['step'], rates, color=colors_rate, alpha=0.7)\n        ax.set_xlabel(\"Update Step\", fontsize=11)\n        ax.set_ylabel(\"Δ(Frobenius Norm)\", fontsize=11)\n        ax.set_title(\"Rate of Symmetry Breaking\\n(green = differentiating, blue = converging)\", \n                     fontsize=12)\n        ax.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Summary\n    print(\"Symmetry breaking summary:\")\n    print(f\"  Initial spread: {sb_df['frobenius'].iloc[0]:.4f}\")\n    print(f\"  Final spread:   {sb_df['frobenius'].iloc[-1]:.4f}\")\n    print(f\"  Max rate of change: {rates.abs().max():.4f} at step {rates.abs().argmax()}\")\n    \n    # Is it gradual or sharp?\n    if len(sb_df) > 2:\n        mid = len(sb_df) // 2\n        early_rate = sb_df['frobenius'].iloc[mid] - sb_df['frobenius'].iloc[0]\n        late_rate = sb_df['frobenius'].iloc[-1] - sb_df['frobenius'].iloc[mid]\n        if early_rate > 0 and late_rate / early_rate < 0.5:\n            print(\"  Pattern: SHARP early differentiation, then plateau (first-order-like)\")\n        elif early_rate > 0:\n            print(\"  Pattern: Gradual differentiation (second-order-like)\")\n        else:\n            print(\"  Pattern: Insufficient data for classification\")\nelse:\n    print(\"Need 2+ trajectories for symmetry-breaking analysis.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 12.4 What the Symmetries Tell Us (and What We Want to Know Next)\n\n**Verified today:**\n\n| Symmetry | Status | Implication |\n|----------|--------|-------------|\n| Conjugation (α ↔ β) | Exact | Manifold is mirror-symmetric about the diagonal. Success and failure are geometrically interchangeable. |\n| Scaling (kα, kβ) | ~k⁻² | The manifold compresses as evidence grows. Diminishing returns are geometric, not a heuristic. |\n| Permutation (arm identity) | Breaks with evidence | All arms start equal. Symmetry-breaking pattern (sharp vs gradual) classifies the learning regime. |\n\n**What we want to know (requires more math):**\n\n1. **Full symmetry group**: are there symmetries beyond conjugation, scaling, and time-translation? Each one we find = one more conserved quantity for interoception.\n\n2. **Lie algebra structure**: the infinitesimal generators of the symmetry group form a Lie algebra. Its dimension = the number of independent conserved quantities. Its structure constants = how the symmetries interact.\n\n3. **Phase transition classification**: the symmetry-breaking curve from 12.3 is either sharp (first-order, like ice melting) or gradual (second-order, like a magnet losing magnetization). This determines whether the system \"snaps\" to a conclusion or gradually drifts. The curvature at the transition point predicts which.\n\n4. **Representation decomposition**: the symmetry group acts on the space of all possible belief states. Decomposing this action into irreducible representations tells us the \"modes\" of the system: independent channels of information that can be learned separately. Hidden modes = structural blind spots.\n\n**The punchline for interoception:** every row in the \"Verified today\" table is a conserved quantity the system can monitor. Every row in \"What we want to know\" is a POTENTIAL conserved quantity we haven't found yet. The full table, once complete, IS the specification of the interoception layer's sensors.\n\n**Roadmap connections:**\n- Phase 3 (dynamical systems) gives fixed points and stability → enables 11.1, 11.2\n- Phase 4 (curvature) gives K(α,β) → enables phase transition classification\n- Phase 5 (Noether/Hamiltonian) gives the conserved quantities → enables full interoception spec\n- Phase 7 (group theory) gives the Lie algebra → enables representation decomposition\n- Aegir Arc 3 Module 3.3-3.4 → the mathematical training for all of the above\n- Aegir Arc 4 → category-theoretic formalization (functors, natural transformations)",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}